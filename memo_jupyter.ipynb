{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/memo-jupyter/blob/main/memo_jupyter.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjYy0F2gZIPR"
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 torchtext==0.18.0 torchdata==0.8.0 --extra-index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install xformers==0.0.28.post3\n",
    "\n",
    "!pip install accelerate==1.1.1 albumentations==1.4.21 audio-separator==0.24.1 black==23.12.1 diffusers==0.31.0 einops==0.8.0 ffmpeg-python==0.2.0 funasr==1.0.27 huggingface-hub==0.26.2 imageio==2.36.0\n",
    "!pip install imageio-ffmpeg==0.5.1 insightface==0.7.3 hydra-core==1.3.2 jax==0.4.35 mediapipe==0.10.18 modelscope==1.20.1 moviepy==1.0.3 numpy==1.26.4 omegaconf==2.3.0 onnxruntime-gpu==1.20.1\n",
    "!pip install opencv-python-headless==4.10.0.84 pillow==10.4.0 scikit-learn==1.5.2 scipy==1.14.1 transformers==4.46.3 tqdm==4.67.1 matplotlib matplotlib-inline\n",
    "\n",
    "!apt install aria2 ffmpeg -qqy\n",
    "\n",
    "%cd /workspace\n",
    "!git clone https://github.com/memoavatar/memo\n",
    "%cd /workspace/memo\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/audio_proj/config.json -d /workspace/memo/checkpoints/audio_proj -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/audio_proj/diffusion_pytorch_model.safetensors -d /workspace/memo/checkpoints/audio_proj -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/diffusion_net/config.json -d /workspace/memo/checkpoints/diffusion_net -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/diffusion_net/diffusion_pytorch_model.safetensors -d /workspace/memo/checkpoints/diffusion_net -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/image_proj/config.json -d /workspace/memo/checkpoints/image_proj -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/image_proj/diffusion_pytorch_model.safetensors -d /workspace/memo/checkpoints/image_proj -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/misc/audio_emotion_classifier/config.json -d /workspace/memo/checkpoints/misc/audio_emotion_classifier -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/audio_emotion_classifier/diffusion_pytorch_model.safetensors -d /workspace/memo/checkpoints/misc/audio_emotion_classifier -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/face_analysis/models/1k3d68.onnx -d /workspace/memo/checkpoints/misc/face_analysis/models -o 1k3d68.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/face_analysis/models/2d106det.onnx -d /workspace/memo/checkpoints/misc/face_analysis/models -o 2d106det.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/face_analysis/models/face_landmarker_v2_with_blendshapes.task -d /workspace/memo/checkpoints/misc/face_analysis/models -o face_landmarker_v2_with_blendshapes.task\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/face_analysis/models/genderage.onnx -d /workspace/memo/checkpoints/misc/face_analysis/models -o genderage.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/face_analysis/models/glintr100.onnx -d /workspace/memo/checkpoints/misc/face_analysis/models -o glintr100.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/face_analysis/models/scrfd_10g_bnkps.onnx -d /workspace/memo/checkpoints/misc/face_analysis/models -o scrfd_10g_bnkps.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/vocal_separator/Kim_Vocal_2.onnx -d /workspace/memo/checkpoints/misc/vocal_separator -o Kim_Vocal_2.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/misc/vocal_separator/download_checks.json -d /workspace/memo/checkpoints/misc/vocal_separator -o download_checks.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/misc/vocal_separator/mdx_model_data.json -d /workspace/memo/checkpoints/misc/vocal_separator -o mdx_model_data.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/misc/vocal_separator/vr_model_data.json -d /workspace/memo/checkpoints/misc/vocal_separator -o vr_model_data.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/reference_net/config.json -d /workspace/memo/checkpoints/reference_net -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/reference_net/diffusion_pytorch_model.safetensors -d /workspace/memo/checkpoints/reference_net -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/stabilityai/sd-vae-ft-mse/resolve/main/diffusion_pytorch_model.safetensors -d /workspace/memo/checkpoints/vae -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/stabilityai/sd-vae-ft-mse/raw/main/config.json -d /workspace/memo/checkpoints/vae -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/model.safetensors -d /workspace/memo/checkpoints/wav2vec2 -o model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/raw/main/config.json -d /workspace/memo/checkpoints/wav2vec2 -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/raw/main/preprocessor_config.json -d /workspace/memo/checkpoints/wav2vec2 -o preprocessor_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/emotion2vec/emotion2vec_plus_large/resolve/main/model.pt -d /workspace/memo/checkpoints/emotion2vec_plus_large -o model.pt\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/emotion2vec/emotion2vec_plus_large/raw/main/config.yaml -d /workspace/memo/checkpoints/emotion2vec_plus_large -o config.yaml\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/emotion2vec/emotion2vec_plus_large/raw/main/configuration.json -d /workspace/memo/checkpoints/emotion2vec_plus_large -o configuration.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/emotion2vec/emotion2vec_plus_large/raw/main/tokens.txt -d /workspace/memo/checkpoints/emotion2vec_plus_large -o tokens.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/memo\n",
    "\n",
    "import os, torch\n",
    "from diffusers import AutoencoderKL, FlowMatchEulerDiscreteScheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from memo.models.audio_proj import AudioProjModel\n",
    "from memo.models.image_proj import ImageProjModel\n",
    "from memo.models.unet_2d_condition import UNet2DConditionModel\n",
    "from memo.models.unet_3d import UNet3DConditionModel\n",
    "from memo.pipelines.video_pipeline import VideoPipeline\n",
    "from memo.utils.audio_utils import extract_audio_emotion_labels, preprocess_audio, resample_audio\n",
    "from memo.utils.vision_utils import preprocess_image, tensor_to_video\n",
    "\n",
    "config = {\n",
    "    \"resolution\": 512,\n",
    "    \"num_generated_frames_per_clip\": 16,\n",
    "    \"fps\": 30,\n",
    "    \"num_init_past_frames\": 2,\n",
    "    \"num_past_frames\": 16,\n",
    "    \"inference_steps\": 20,\n",
    "    \"cfg_scale\": 3.5,\n",
    "    \"weight_dtype\": \"bf16\",\n",
    "    \"model_name_or_path\": \"/workspace/memo/checkpoints\",\n",
    "    \"vae\": \"/workspace/memo/checkpoints/vae\",\n",
    "    \"wav2vec\": \"/workspace/memo/checkpoints/wav2vec2\",\n",
    "    \"emotion2vec\": \"/workspace/memo/checkpoints/emotion2vec_plus_large\",\n",
    "    \"misc_model_dir\": \"/workspace/memo/checkpoints\",\n",
    "}\n",
    "\n",
    "seed = 42\n",
    "input_image_path = \"assets/examples/dicaprio.jpg\"\n",
    "input_audio_path = \"assets/examples/speech.wav\"\n",
    "output_dir = \"outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_video_path = os.path.join(output_dir, f\"{os.path.basename(input_image_path).split('.')[0]}_{os.path.basename(input_audio_path).split('.')[0]}.mp4\")\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "weight_dtype = {\"fp16\": torch.float16, \"bf16\": torch.bfloat16, \"fp32\": torch.float32}.get(config[\"weight_dtype\"], torch.float32)\n",
    "\n",
    "face_analysis = os.path.join(config[\"misc_model_dir\"], \"misc/face_analysis\")\n",
    "vocal_separator = os.path.join(config[\"misc_model_dir\"], \"misc/vocal_separator/Kim_Vocal_2.onnx\")\n",
    "\n",
    "img_size = (config[\"resolution\"], config[\"resolution\"])\n",
    "pixel_values, face_emb = preprocess_image(\n",
    "    face_analysis_model=face_analysis, image_path=input_image_path, image_size=config[\"resolution\"]\n",
    ")\n",
    "\n",
    "cache_dir = os.path.join(output_dir, \"audio_preprocess\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "input_audio_path = resample_audio(input_audio_path, os.path.join(cache_dir, f\"{os.path.basename(input_audio_path).split('.')[0]}-16k.wav\"))\n",
    "\n",
    "audio_emb, audio_length = preprocess_audio(\n",
    "    wav_path=input_audio_path,\n",
    "    num_generated_frames_per_clip=config[\"num_generated_frames_per_clip\"],\n",
    "    fps=config[\"fps\"],\n",
    "    wav2vec_model=config[\"wav2vec\"],\n",
    "    vocal_separator_model=vocal_separator,\n",
    "    cache_dir=cache_dir,\n",
    "    device=device,\n",
    ")\n",
    "audio_emotion, num_emotion_classes = extract_audio_emotion_labels(\n",
    "    model=config[\"model_name_or_path\"],\n",
    "    wav_path=input_audio_path,\n",
    "    emotion2vec_model=config[\"emotion2vec\"],\n",
    "    audio_length=audio_length,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(config[\"vae\"]).to(device=device, dtype=weight_dtype)\n",
    "reference_net = UNet2DConditionModel.from_pretrained(config[\"model_name_or_path\"], subfolder=\"reference_net\", use_safetensors=True)\n",
    "diffusion_net = UNet3DConditionModel.from_pretrained(config[\"model_name_or_path\"], subfolder=\"diffusion_net\", use_safetensors=True)\n",
    "image_proj = ImageProjModel.from_pretrained(config[\"model_name_or_path\"], subfolder=\"image_proj\", use_safetensors=True)\n",
    "audio_proj = AudioProjModel.from_pretrained(config[\"model_name_or_path\"], subfolder=\"audio_proj\", use_safetensors=True)\n",
    "\n",
    "vae.requires_grad_(False).eval()\n",
    "reference_net.requires_grad_(False).eval()\n",
    "diffusion_net.requires_grad_(False).eval()\n",
    "image_proj.requires_grad_(False).eval()\n",
    "audio_proj.requires_grad_(False).eval()\n",
    "\n",
    "reference_net.enable_xformers_memory_efficient_attention()\n",
    "diffusion_net.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "noise_scheduler = FlowMatchEulerDiscreteScheduler()\n",
    "pipeline = VideoPipeline(\n",
    "    vae=vae,\n",
    "    reference_net=reference_net,\n",
    "    diffusion_net=diffusion_net,\n",
    "    scheduler=noise_scheduler,\n",
    "    image_proj=image_proj,\n",
    ")\n",
    "pipeline.to(device=device, dtype=weight_dtype)\n",
    "\n",
    "video_frames = []\n",
    "num_clips = audio_emb.shape[0] // config[\"num_generated_frames_per_clip\"]\n",
    "for t in tqdm(range(num_clips), desc=\"Generating video clips\"):\n",
    "    if len(video_frames) == 0:\n",
    "        past_frames = pixel_values.repeat(config[\"num_init_past_frames\"], 1, 1, 1).to(dtype=pixel_values.dtype, device=pixel_values.device)\n",
    "        pixel_values_ref_img = torch.cat([pixel_values, past_frames], dim=0)\n",
    "    else:\n",
    "        past_frames = video_frames[-1][0].permute(1, 0, 2, 3)[-config[\"num_past_frames\"]:] * 2.0 - 1.0\n",
    "        past_frames = past_frames.to(dtype=pixel_values.dtype, device=pixel_values.device)\n",
    "        pixel_values_ref_img = torch.cat([pixel_values, past_frames], dim=0)\n",
    "\n",
    "    pixel_values_ref_img = pixel_values_ref_img.unsqueeze(0)\n",
    "    audio_tensor = (\n",
    "        audio_emb[t * config[\"num_generated_frames_per_clip\"] : min((t + 1) * config[\"num_generated_frames_per_clip\"], audio_emb.shape[0])]\n",
    "        .unsqueeze(0)\n",
    "        .to(device=audio_proj.device, dtype=audio_proj.dtype)\n",
    "    )\n",
    "    audio_tensor = audio_proj(audio_tensor)\n",
    "    audio_emotion_tensor = audio_emotion[\n",
    "        t * config[\"num_generated_frames_per_clip\"] : min((t + 1) * config[\"num_generated_frames_per_clip\"], audio_emb.shape[0])\n",
    "    ]\n",
    "\n",
    "    pipeline_output = pipeline(\n",
    "        ref_image=pixel_values_ref_img,\n",
    "        audio_tensor=audio_tensor,\n",
    "        audio_emotion=audio_emotion_tensor,\n",
    "        emotion_class_num=num_emotion_classes,\n",
    "        face_emb=face_emb,\n",
    "        width=img_size[0],\n",
    "        height=img_size[1],\n",
    "        video_length=config[\"num_generated_frames_per_clip\"],\n",
    "        num_inference_steps=config[\"inference_steps\"],\n",
    "        guidance_scale=config[\"cfg_scale\"],\n",
    "        generator=torch.manual_seed(seed),\n",
    "    )\n",
    "    video_frames.append(pipeline_output.videos)\n",
    "\n",
    "video_frames = torch.cat(video_frames, dim=2).squeeze(0)[:, :audio_length]\n",
    "tensor_to_video(video_frames, output_video_path, input_audio_path, fps=config[\"fps\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
